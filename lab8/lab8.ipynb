{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from pandas import get_dummies, DataFrame\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from utils import load_data\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Your initialization procedure if required\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def create_dict(self, x):\n",
    "        dct = dict()\n",
    "        if type(x[0]) == float:\n",
    "            dct['mean'], dct['sigma'] = self.estimate_mean_and_stdev(x)\n",
    "        else:\n",
    "            for unq_v in np.unique(x):\n",
    "                dct[unq_v] = (x==unq_v).sum()\n",
    "        return dct\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        This method calculates class probabilities and conditional probabilities to be used for prediction\n",
    "\n",
    "        Both numerical and categorical features are accepted.\n",
    "        Conditional probability of numerical features is calculated based on Probability Density Function\n",
    "        (assuming normal distribution)\n",
    "\n",
    "        :param X: training data, numpy array of shape (n,m)\n",
    "        :param Y: training labels, numpy array of shape (n,1)\n",
    "        \"\"\"\n",
    "        # TODO START YOUR CODE HERE\n",
    "        self.m_pos = Y[Y[:,0]=='+'].size\n",
    "        self.m_neg = Y[Y[:,0]=='-'].size\n",
    "        self.n = X.shape[1]\n",
    "        self.pos_proba = (Y=='+').mean()\n",
    "        self.neg_proba = 1-self.pos_proba\n",
    "        self.pos_dicts = [self.create_dict(X[:,i][Y[:,0]=='+']) for i in range(X.shape[1])]\n",
    "        self.neg_dicts = [self.create_dict(X[:,i][Y[:,0]=='-']) for i in range(X.shape[1])]\n",
    "        # END YOUR CODE HERE\n",
    "\n",
    "    @staticmethod\n",
    "    def estimate_mean_and_stdev(values):\n",
    "        \"\"\"\n",
    "        Estimates parameters of normal distribution - empirical mean and standard deviation\n",
    "        :param values: attribute sample values\n",
    "        :return: mean, stdev\n",
    "        \"\"\"\n",
    "        # TODO START YOUR CODE HERE\n",
    "        m = values.size\n",
    "        mean = values.sum()/m\n",
    "        std = ((values-mean).dot(values-mean)/m)**.5\n",
    "        return mean, std\n",
    "        # END YOUR CODE HERE\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_probability(val, mean, stdev):\n",
    "        \"\"\"\n",
    "        Estimates probability of encountering a point (val) given parameters of normal distribution\n",
    "        based on probability density function\n",
    "        :param val: point\n",
    "        :param mean: mean value\n",
    "        :param stdev: standard deviation\n",
    "        :return: relative likelihood of a point\n",
    "        \"\"\"\n",
    "        # TODO START YOUR CODE HERE\n",
    "        return np.sqrt(2*np.pi*stdev**2)**-1*np.e**-((val-mean)**2/2/stdev**2)\n",
    "        # END YOUR CODE HERE\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for given input. Refer to lecture slides for corresponding formula\n",
    "        :param X: test data, numpy array of shape (n,m)\n",
    "        :return: numpy array of predictions\n",
    "        \"\"\"\n",
    "        # TODO START YOUR CODE HERE\n",
    "        probas = []\n",
    "        alpha = 1\n",
    "        for x in X:\n",
    "            pos = sum([np.log((self.pos_dicts[i][x[i]]+alpha)/(self.m_pos+alpha*self.n)) if x[i] in self.pos_dicts[i] else np.log(alpha*(self.m_pos+alpha*self.n)**-1) if type(x[i])!=float else np.log(self.calc_probability(x[i], self.pos_dicts[i]['mean'], self.pos_dicts[i]['sigma'])) for i in range(x.size)])\n",
    "            neg = sum([np.log((self.neg_dicts[i][x[i]]+alpha)/(self.m_neg+alpha*self.n)) if x[i] in self.neg_dicts[i] else np.log(alpha*(self.m_neg+alpha*self.n)**-1) if type(x[i])!=float else np.log(self.calc_probability(x[i], self.pos_dicts[i]['mean'], self.pos_dicts[i]['sigma'])) for i in range(x.size)])\n",
    "            probas += '+' if pos>neg else'-'\n",
    "        return np.array(probas)\n",
    "        # END YOUR CODE HERE\n",
    "\n",
    "    def get_params(self, deep = False):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = load_data(\"crx.data.csv\")\n",
    "# indexes of numerical attributes\n",
    "numerical_attrs = [1, 2, 7, 10, 13, 14]\n",
    "X[:, numerical_attrs] = X[:, numerical_attrs].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# categorical features only. Use this to test your initial implementation\n",
    "X_cat = np.delete(X, numerical_attrs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NaiveBayes()\n",
    "clf.fit(X, Y)\n",
    "Y_pred = clf.predict(X[[0],:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Accuracy: 0.86 (+/- 0.09)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(NaiveBayes(), X_cat, Y, cv=KFold(n_splits=15, shuffle=True), scoring='accuracy')\n",
    "print(\"Categorical Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Accuracy of Standard NB: 0.86 (+/- 0.09)\n"
     ]
    }
   ],
   "source": [
    "# use this as a benchmark. Your algorithm (on categorical features) should reach the same accuracy\n",
    "X_dummy = DataFrame.as_matrix(get_dummies(DataFrame(X_cat)))\n",
    "scores = cross_val_score(MultinomialNB(), X_dummy, Y.ravel(), cv=KFold(n_splits=15, shuffle=True), scoring='accuracy')\n",
    "print(\"Categorical Accuracy of Standard NB: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.85 (+/- 0.12)\n"
     ]
    }
   ],
   "source": [
    "# all (mixed) features. Use this to test your final implementation\n",
    "scores = cross_val_score(NaiveBayes(), X, Y, cv=KFold(n_splits=15, shuffle=True), scoring='accuracy')\n",
    "print(\"Overall Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "# write your thoughts here (if any)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
