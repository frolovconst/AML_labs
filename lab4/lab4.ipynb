{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path_to_csv, has_header=True):\n",
    "    \"\"\"\n",
    "    Loads a csv file, the last column is assumed to be the output label\n",
    "    All values are interpreted as strings, empty cells interpreted as empty\n",
    "    strings\n",
    "\n",
    "    returns: X - numpy array of size (n,m) of input features\n",
    "             Y - numpy array of output features\n",
    "    \"\"\"\n",
    "    if has_header:\n",
    "        data = read_csv(path_to_csv, header='infer', dtype=str)\n",
    "    else:\n",
    "        data = read_csv(path_to_csv, header=None, dtype=str)\n",
    "    data.fillna('', inplace=True)\n",
    "    data = data.as_matrix()\n",
    "    X = data[:, :-1]\n",
    "    Y = data[:, -1]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_train_test_split(X, Y, fraction):\n",
    "    \"\"\"\n",
    "    perform the split of the data into training and testing sets\n",
    "    input:\n",
    "        X: numpy array of size (n,m)\n",
    "        Y: numpy array of size (n,)\n",
    "        fraction: number between 0 and 1, specifies the size of the training\n",
    "                data\n",
    "\n",
    "    returns:\n",
    "        X_train\n",
    "        Y_train\n",
    "        X_test\n",
    "        Y_test\n",
    "    \"\"\"\n",
    "    if fraction < 0 or fraction > 1:\n",
    "        raise Exception(\"Fraction for split is not valid\")\n",
    "\n",
    "    # do random sampling for splitting the data\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=fraction)\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DTree:\n",
    "    \"\"\"\n",
    "    Simple decision tree classifier for a training data with categorical\n",
    "    features\n",
    "    \"\"\"\n",
    "    _model = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self._model = create_branches({'attr_id': -1,\n",
    "                                       'branches': dict(),\n",
    "                                       'decision': None}, X, Y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError\n",
    "\n",
    "        if X.ndim == 1:\n",
    "            return None\n",
    "        elif X.ndim == 2:\n",
    "            return None\n",
    "        else:\n",
    "            print(\"Dimensions error\")\n",
    "\n",
    "    def prune(self):\n",
    "        \"\"\"\n",
    "        Implement pruning to improve generalization\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elem_to_freq(values):\n",
    "    \"\"\"\n",
    "    input: numpy array\n",
    "    returns: The counts of unique elements, unique elements are not returned\n",
    "    \"\"\"\n",
    "    # hint: check numpy documentation for how to count unique values\n",
    "    classes = np.unique(values)\n",
    "    counts = np.array([values[values==classes[i]].size for i in range(classes.size)])\n",
    "#     raise NotImplementedError\n",
    "    return counts/values.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(elements):\n",
    "    \"\"\"\n",
    "    Calculates entropy of a numpy array of instances\n",
    "    input: numpy array\n",
    "    returns: entropy of the input array based on the frequencies of observed\n",
    "             elements\n",
    "    \"\"\"\n",
    "    # hint: use elem_to_freq(arr)\n",
    "#     raise NotImplementedError\n",
    "    freq = elem_to_freq(elements)\n",
    "    return -(freq.dot(np.log2(freq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def information_gain(A, X, Y): #S):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        A: the values of an attribute A for the set of training examples\n",
    "        S: the target output class\n",
    "\n",
    "    returns: information gain for classifying using the attribute A\n",
    "    \"\"\"\n",
    "    # hint: use entropy(arr)\\\n",
    "    Ent_S = entropy(Y)\n",
    "    attr_vals = np.unique(X[:,A])\n",
    "    ents = np.array([entropy(Y[X[:,A]==val])*Y[X[:,A]==val].size for val in attr_vals])/Y.size\n",
    "#     raise NotImplementedError\n",
    "    return entropy(Y) - ents.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_best_attribute(X, Y):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        X: numpy array of size (n,m) containing training examples\n",
    "        Y: numpy array of size (n,) containing target class\n",
    "\n",
    "    returns: the index of the attribute that results in maximum information\n",
    "             gain. If maximum information gain is less that eps, returns -1\n",
    "    \"\"\"\n",
    "\n",
    "    eps = 1e-10\n",
    "    max_gain = 0\n",
    "    best_attr = 0\n",
    "    for i in range(X.shape[1]):\n",
    "        c_gain = information_gain(i, X, Y)\n",
    "        if c_gain > max_gain:\n",
    "            max_gain = c_gain\n",
    "            best_attr = i\n",
    "\n",
    "#     raise NotImplementedError\n",
    "    return best_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_common_class(Y):\n",
    "    \"\"\"\n",
    "    input: target class values\n",
    "    returns: the value of the most common class\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_branches(node, X, Y):\n",
    "    \"\"\"\n",
    "    create branches in a decision tree recursively\n",
    "    input:\n",
    "        node: current node represented by a dictionary of format\n",
    "                {'attr_id': -1,\n",
    "                 'branches': dict(),\n",
    "                 'decision': None},\n",
    "              where attr_id: specifies the current attribute index for branching\n",
    "                            -1 mean the node is leaf node\n",
    "                    braches: is a dictionary of format {attr_val:node}\n",
    "                    decision: contains either the best guess based on\n",
    "                            most common class or an actual class label if the\n",
    "                            current node is the leaf\n",
    "        X: training examples\n",
    "        Y: target class\n",
    "\n",
    "    returns: input node with fields updated\n",
    "    \"\"\"\n",
    "    # choose best attribute to branch\n",
    "    attr_id = None\n",
    "    node['attr_id'] = attr_id\n",
    "    # record the most common class\n",
    "    node['decision'] = None\n",
    "\n",
    "    if attr_id != -1:\n",
    "        # find the set of unique values for the current attribute\n",
    "        attr_vals = None\n",
    "\n",
    "        for a_val in attr_vals:\n",
    "            # compute the boolean array for slicing the data for the next\n",
    "            # branching iteration\n",
    "            # hint: use logical operation on numpy array\n",
    "            # for more information about slicing refer to numpy documentation\n",
    "            sel = None\n",
    "            # perform slicing\n",
    "            X_branch = X[sel, :]\n",
    "            Y_branch = Y[sel]\n",
    "            node_template = {'attr_id': -1,\n",
    "                             'branches': dict(),\n",
    "                             'decision': None}\n",
    "            # perform recursive call\n",
    "            node['branches'][a_val] = None\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def traverse(model,sample):\n",
    "    \"\"\"\n",
    "    recursively traverse decision tree\n",
    "    input:\n",
    "        model: trained decision tree\n",
    "        sample: input sample to classify\n",
    "\n",
    "    returns: class label\n",
    "    \"\"\"\n",
    "    if model['attr_id'] == -1:\n",
    "        decision = model['decision']\n",
    "    else:\n",
    "        attr_val = sample[ model['attr_id'] ]\n",
    "        if attr_val not in model['branches']:\n",
    "            decision = model['decision']\n",
    "        else:\n",
    "            decision = traverse(model['branches'][attr_val], sample)\n",
    "    return decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def measure_error(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "    returns an error measure of your choice\n",
    "    \"\"\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recall(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "    returns recall value\n",
    "    \"\"\"\n",
    "    return None\n",
    "\n",
    "\n",
    "# 1.  test your implementation on data_1.csv\n",
    "#     refer to lecture slides to verify the correctness\n",
    "# 2.  test your implementation on mushrooms_modified.csv\n",
    "# 3.  test your implementation on titanic_modified.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = load_data(\"data_1.csv\")\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = my_train_test_split(X, Y, .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "d_tree = DTree()\n",
    "d_tree.fit(X_train,Y_train)\n",
    "Y_pred = d_tree.predict(X_test)\n",
    "print(\"Correctly classified: %.2f%%\" % (measure_error(Y_test,Y_pred) * 100))\n",
    "print(\"Recall %.4f\" % recall(Y_test, Y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
